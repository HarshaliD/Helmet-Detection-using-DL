{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarshaliD/Helmet-Detection-using-DL/blob/main/helmetDetection_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d72fcca-34b9-42c5-b51c-4cbe29b3540c",
      "metadata": {
        "id": "3d72fcca-34b9-42c5-b51c-4cbe29b3540c"
      },
      "outputs": [],
      "source": [
        " import mrcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f85afe28-6a68-46e3-a0fc-053a0f3966e7",
      "metadata": {
        "id": "f85afe28-6a68-46e3-a0fc-053a0f3966e7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow.keras.layers as KL\n",
        "import tensorflow.keras.models as KM\n",
        "from numpy import zeros, asarray\n",
        "from xml.etree import ElementTree\n",
        "from mrcnn.utils import Dataset, extract_bboxes\n",
        "from mrcnn.visualize import display_instances\n",
        "from matplotlib import pyplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f716a92c-2b62-4200-8ea1-e605e29edbd8",
      "metadata": {
        "id": "f716a92c-2b62-4200-8ea1-e605e29edbd8"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "image_path = \"C:\\\\Users\\\\lenovo\\\\Desktop\\\\archive (16)\\\\images\\\\0014.png\"\n",
        "\n",
        "# Read the image using OpenCV\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# Check if image was read successfully\n",
        "if image is not None:\n",
        "  # Get the image size (height, width)\n",
        "  height, width, channels = image.shape\n",
        "\n",
        "  # Print the image size\n",
        "  print(f\"Image size: (height, width) = ({height}, {width})\")\n",
        "else:\n",
        "  print(f\"Error: Could not read image at path: {image_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff54700b-f6b0-4d83-ba9f-1eaa187b83a0",
      "metadata": {
        "id": "ff54700b-f6b0-4d83-ba9f-1eaa187b83a0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d18e2fe-fdff-4547-9bba-1dbd30955b45",
      "metadata": {
        "id": "5d18e2fe-fdff-4547-9bba-1dbd30955b45"
      },
      "outputs": [],
      "source": [
        "class FruitsDataset(Dataset):\n",
        "    # load the dataset definitions\n",
        "    def load_dataset_annot(self, dataset_dir, is_train=True):\n",
        "        # define classes\n",
        "        self.add_class(\"dataset\", 1, \"helmet\")\n",
        "\n",
        "        # define data locations\n",
        "        images_dir = os.path.join(dataset_dir, 'images')\n",
        "        annotations_dir = os.path.join(\"C:\\\\Users\\\\lenovo\\\\Desktop\\\\archive (16)\", 'annotations')\n",
        "\n",
        "        # find all images\n",
        "        for filename in os.listdir(annotations_dir):\n",
        "            print(filename)\n",
        "\t\t\t# extract image id\n",
        "            image_id = filename.split()[0][:4]\n",
        "\t\t\t#print('IMAGE ID: ',image_id)\n",
        "\n",
        "\t\t\t# skip all images after 115 if we are building the train set\n",
        "            if is_train and int(image_id) >= 2000:\n",
        "                continue\n",
        "\t\t\t# skip all images before 115 if we are building the test/val set\n",
        "            if not is_train and int(image_id) < 2000:\n",
        "                continue\n",
        "            img_path = images_dir + filename\n",
        "            ann_path = annotations_dir + image_id + '.xml'\n",
        "\t\t\t# add to dataset\n",
        "            self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path, class_ids = [1])\n",
        "\n",
        "\n",
        "    def extract_boxes(self, filename, resize_factor=(1.0, 1.0)):\n",
        "    # Load and parse the file\n",
        "        tree = ElementTree.parse(filename)\n",
        "        # Get the root of the document\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Extract each bounding box\n",
        "        boxes = list()\n",
        "        for box in root.findall('.//object'):\n",
        "            name = box.find('name').text  # Add label name to the box list\n",
        "            xmin = int(box.find('./bndbox/xmin').text)\n",
        "            ymin = int(box.find('./bndbox/ymin').text)\n",
        "            xmax = int(box.find('./bndbox/xmax').text)\n",
        "            ymax = int(box.find('./bndbox/ymax').text)\n",
        "\n",
        "            # Resize coordinates based on the factors\n",
        "            resized_xmin = int(xmin * resize_factor[0])\n",
        "            resized_ymin = int(ymin * resize_factor[1])\n",
        "            resized_xmax = int(xmax * resize_factor[0])\n",
        "            resized_ymax = int(ymax * resize_factor[1])\n",
        "\n",
        "            coors = [resized_xmin, resized_ymin, resized_xmax, resized_ymax, name]\n",
        "            boxes.append(coors)\n",
        "\n",
        "        # Extract original image dimensions\n",
        "        width = int(root.find('.//size/width').text)\n",
        "        height = int(root.find('.//size/height').text)\n",
        "\n",
        "        return boxes, width, height\n",
        "\n",
        "\n",
        "    def get_label(self, image_path):\n",
        "        \"\"\"\n",
        "      Extracts labels and image dimensions from the corresponding XML annotation file.\n",
        "\n",
        "      Args:\n",
        "        image_path (str): Path to the image file.\n",
        "\n",
        "      Returns:\n",
        "        list: List of labels extracted from the XML file.\n",
        "        int: Width of the original image.\n",
        "        int: Height of the original image.\n",
        "      \"\"\"\n",
        "        filename, _ = os.path.splitext(os.path.basename(image_path))\n",
        "        annotation_file = os.path.join(\"C:\\\\Users\\\\lenovo\\\\Desktop\\\\archive (16)\\\\annotations\", filename + '.xml')\n",
        "\n",
        "        if not os.path.exists(annotation_file):\n",
        "            raise FileNotFoundError(f\"Annotation file not found: {annotation_file}\")\n",
        "\n",
        "        try:\n",
        "            # Calculate resize factor dynamically based on target size and original image dimensions\n",
        "            original_width, original_height = get_image_dimensions(image_path)\n",
        "            resize_factor_w = target_size[0] / original_width\n",
        "            resize_factor_h = target_size[1] / original_height\n",
        "\n",
        "            # Use extract_boxes to get labels and image dimensions\n",
        "            boxes, width, height = self.extract_boxes(annotation_file, resize_factor=(resize_factor_w, resize_factor_h))\n",
        "\n",
        "            # Return labels, original width, and height\n",
        "            labels = [box[-1] for box in boxes]  # Extract only labels from bounding boxes\n",
        "            return labels, original_width, original_height\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing annotation file: {annotation_file} - {e}\")\n",
        "            return [], 0, 0  # Handle parsing errors (optional)\n",
        "\n",
        "    import random\n",
        "\n",
        "    def load_dataset(self, dataset_dir, val_ratio=0.2, test_ratio=0.1):\n",
        "        \"\"\"\n",
        "        Loads dataset images and annotations, splitting into train, validation, and test sets.\n",
        "\n",
        "        Args:\n",
        "            dataset_dir (str): Path to the dataset directory.\n",
        "            val_ratio (float, optional): Ratio for splitting the dataset into validation (default: 0.2).\n",
        "            test_ratio (float, optional): Ratio for splitting the dataset into test (default: 0.1).\n",
        "        \"\"\"\n",
        "\n",
        "        # Define classes\n",
        "        self.add_class(\"dataset\", 1, \"helmet\")\n",
        "\n",
        "        # Define data locations\n",
        "        images_dir = os.path.join(dataset_dir, 'images')\n",
        "        annotations_dir = os.path.join(\"C:\\\\Users\\\\lenovo\\\\Downloads\\\\archive (16)\", 'annotations')\n",
        "\n",
        "        all_image_ids = []\n",
        "        train_image_ids = []\n",
        "        val_image_ids = []\n",
        "        test_image_ids = []\n",
        "        train_annotated = []\n",
        "        val_annotated = []\n",
        "        test_annotated = []\n",
        "\n",
        "        # Find all images\n",
        "        for filename in os.listdir(images_dir):\n",
        "            image_id = filename.split(\"_\", 1)[0][:4]\n",
        "            all_image_ids.append(image_id)\n",
        "\n",
        "        # Split data into train/val/test based on ratios\n",
        "        total_images = len(all_image_ids)\n",
        "        num_test_images = int(total_images * test_ratio)\n",
        "        num_val_images = int(total_images * val_ratio)\n",
        "        num_train_images = total_images - (num_test_images + num_val_images)\n",
        "\n",
        "        # Randomly select image IDs for test and validation sets\n",
        "        test_image_ids = random.sample(all_image_ids, num_test_images)\n",
        "        remaining_ids = [image_id for image_id in all_image_ids if image_id not in test_image_ids]\n",
        "        val_image_ids = random.sample(remaining_ids, num_val_images)\n",
        "        train_image_ids = [image_id for image_id in remaining_ids if image_id not in val_image_ids]\n",
        "\n",
        "        # Process training set images\n",
        "        for image_id in train_image_ids:\n",
        "            img_path = os.path.join(images_dir, image_id + \".png\")  # Adjust image extension if needed\n",
        "            ann_path = os.path.join(annotations_dir, image_id + '.xml')  # Modify annotation path if needed\n",
        "\n",
        "            # Add image to the dataset (assuming your function handles annotations internally)\n",
        "            self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path, class_ids=[1])\n",
        "\n",
        "            # Get labels, width, and height using get_label (assuming it works for your data structure)\n",
        "            labels, width, height = self.get_label(img_path)\n",
        "\n",
        "            # Append image ID, labels, width, and height to train_annotated list\n",
        "            train_annotated.append((image_id, labels, width, height))\n",
        "\n",
        "        # Process validation and test sets with similar logic, storing in separate lists\n",
        "        for image_id in val_image_ids:\n",
        "            img_path = os.path.join(images_dir, image_id + \".png\")\n",
        "            ann_path = os.path.join(annotations_dir, image_id + '.xml')\n",
        "            self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path, class_ids=[1])\n",
        "            labels, width, height = self.get_label(img_path)\n",
        "            val_annotated.append((image_id, labels, width, height))\n",
        "\n",
        "        for image_id in test_image_ids:\n",
        "            img_path = os.path.join(images_dir, image_id + \".png\")\n",
        "            ann_path = os.path.join(annotations_dir, image_id + '.xml')\n",
        "            self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path, class_ids=[1])\n",
        "            labels, width, height = self.get_label(img_path)\n",
        "            test_annotated.append((image_id, labels, width, height))\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        train_annotated_np = np.array(train_annotated, dtype=object)\n",
        "        val_annotated_np = np.array(val_annotated, dtype=object)\n",
        "        test_annotated_np = np.array(test_annotated, dtype=object)\n",
        "\n",
        "        return train_image_ids, val_image_ids, test_image_ids, train_annotated_np, val_annotated_np, test_annotated_np\n",
        "\n",
        "    def load_image(self):\n",
        "            val = \"C:\\\\Users\\\\lenovo\\\\Desktop\\\\archive (16)\\\\images\"\n",
        "            \"\"\"\n",
        "            Loads and preprocesses an image from the given path.\n",
        "            Args:\n",
        "                img_path (str): Path to the image file.\n",
        "            Returns:\n",
        "                numpy.ndarray: Preprocessed image array.\n",
        "            \"\"\"\n",
        "            image = load_img(val, target_size=(224, 224))  # Adjust target size as needed\n",
        "            image = img_to_array(image)\n",
        "            image = image.astype('float32') / 255.0\n",
        "            return image\n",
        "\n",
        "\n",
        "    def load_images_and_labels(self, image_ids, annotations, images_dir):\n",
        "        images = []\n",
        "        labels = []\n",
        "        for annotation in annotations:\n",
        "            image_id, label, width, height = annotation\n",
        "            img_path = os.path.join(images_dir, image_id + \".png\")\n",
        "            image = self.load_image()\n",
        "            images.append(image)\n",
        "            labels.append(label)\n",
        "        return np.array(images, dtype='float32'), np.array(labels, dtype='int32')  # Ensure consistent dtype\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "dataset_dir = \"C:\\\\Users\\\\lenovo\\\\Desktop\\\\archive (16)\"\n",
        "train_set = FruitsDataset(dataset_dir)\n",
        "train_image_ids, val_image_ids, test_image_ids, train_annotated, val_annotated, test_annotated = train_set.load_dataset(dataset_dir,val_ratio=0.2, test_ratio=0.1)\n",
        "train_images, train_labels = train_set.load_images_and_labels(train_image_ids, train_annotated,dataset_dir)\n",
        "val_images, val_labels = train_set.load_images_and_labels(val_image_ids, val_annotated, dataset_dir)\n",
        "# Access training, validation, and test image IDs for further processing\n",
        "print(\"Train:\", len(train_image_ids))\n",
        "print(\"Validation:\", len(val_image_ids))\n",
        "print(\"Test:\", len(test_image_ids))\n",
        "print(\"Test:\", len(test_annotated))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f078c1f-ec3e-499c-afe1-51cea76b229f",
      "metadata": {
        "id": "6f078c1f-ec3e-499c-afe1-51cea76b229f"
      },
      "outputs": [],
      "source": [
        "\n",
        "image_path = \"C:\\\\Users\\\\lenovo\\\\Downloads\\\\archive (14)\\\\images\\\\3946.png\"\n",
        "\n",
        "# Read the image using OpenCV\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# Check if image was read successfully\n",
        "if image is not None:\n",
        "  # Get the image size (height, width)\n",
        "  height, width, channels = image.shape\n",
        "\n",
        "  # Print the image size\n",
        "  print(f\"Image size: (height, width) = ({height}, {width})\")\n",
        "else:\n",
        "  print(f\"Error: Could not read image at path: {image_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b822c295-a98a-455c-85a2-d357bfec9032",
      "metadata": {
        "id": "b822c295-a98a-455c-85a2-d357bfec9032"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Load the pre-trained ResNet50 model with ImageNet weights\n",
        "model = ResNet50(weights='imagenet', include_top=False,input_shape=(224, 224, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92c19c93-9266-485d-baee-6c25c9f42a2b",
      "metadata": {
        "id": "92c19c93-9266-485d-baee-6c25c9f42a2b"
      },
      "outputs": [],
      "source": [
        "print(model.output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25f885ed-f638-4eea-a78a-4e2860826b52",
      "metadata": {
        "id": "25f885ed-f638-4eea-a78a-4e2860826b52"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Flatten,Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cc9810e-cec5-4b59-b87b-4f8fc10df5d6",
      "metadata": {
        "id": "8cc9810e-cec5-4b59-b87b-4f8fc10df5d6"
      },
      "outputs": [],
      "source": [
        "x = model.output\n",
        "x = Flatten()(x)  # Flatten the output (optional)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)  # Assuming binary classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33f548ab-a4aa-4b3d-aa2f-ddd7050e6552",
      "metadata": {
        "id": "33f548ab-a4aa-4b3d-aa2f-ddd7050e6552"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Assuming binary classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ccab9a4-e451-4220-97a0-0a74c6965a1f",
      "metadata": {
        "id": "3ccab9a4-e451-4220-97a0-0a74c6965a1f"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_images, train_labels,\n",
        "                            epochs=10,  # Adjust number of epochs as needed\n",
        "                            batch_size=32,  # Adjust batch size as needed\n",
        "                            validation_data=(val_images, val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c892989-8635-417c-b24a-b9a10814c51e",
      "metadata": {
        "id": "9c892989-8635-417c-b24a-b9a10814c51e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}